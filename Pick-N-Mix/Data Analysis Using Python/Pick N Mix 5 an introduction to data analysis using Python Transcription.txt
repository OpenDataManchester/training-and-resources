So, hey, I'm john. And you may know me from such other events, as I mentioned, in which we talk about data and Python, and other open source data analysis languages. And generally the whole process about using data and analysing it for whatever various purpose. Sam very kindly mentioned, I run my own business at PDFTA. So we focus mainly on helping and personalization for e commerce organisations, which is generally doing things like recommendation systems and personal I think content, stuff like that. So if you happen to own a large e commerce company, give me a shout. Otherwise, you know, give me a shot anyway, the only question is, are you interested in pirate monster have anything to do with Python or data of anything else? Give me a chance. If you do want to give me a shout out to Justin jgr, on Twitter, and on GitHub, or any of the socials I'm on, I'll be on there with this same handle. So without further ado, let's move on to the data analysis business. So jumping the gun a little bit in terms of GitHub, and get a world we're going to try and use going to try and use a system so that everyone else can can get involved in indies if indeed you so wish. So everything we're going to be doing today, and it will be available in these two notebooks on GitHub. And I think Sam is poppin the link into the into the chat, so everyone can find it. And if you're comfortable, you can do pi two notebooks, and then you'll be familiar with what we're doing at the moment, which is just to do piping up books. Getting Started I py and B, and pandas. plotting the iPad dot i py and date book so that everyone else so that you can follow along without having to faff about with local environments and installing packages and stuff like that we're going to be using at least I'm going to be using the the Google cola project projects products, what have you, which is basically an openly available two python environment which you can use so long as you as long as you're willing to sign up for a Google accounts. And it's pretty useful because it means you don't have to run it on your own computer. And also it lets you use that hardware. So if you're doing some some more advanced techniques, like training in your network, where you'd get an advantage for music and graphics, a graphics card rather than just the regular CPU, and are even more advanced by Kevin's tensor tensor processing unit, so with GPU and CPU can use them for free. And there are limits. So how to do you know any kind of production or commercial work on there, but it's fine for playing around. So there we go. So, and when you open colab, don't research Google, find this little thing popping up for you. I think it'll start on the recent top. If you go along to the get hooked up, and your top bar, you paste in this URL, I suppose. Hit the search bar, and you'll come up with these two notebooks. So we're going to start with a getting started notebook. And it should be a shot. And we've got ourselves an interactive Python environment. So just to demonstrate just how interactive it is. Let's do the very first thing anyone ever does in Python, which is super boring, and let's print out hello world. Oh, So I'm going to be doing some live coding, you're gonna see an awful lot of typos. I hope you're all okay with that. Yeah, what the first time you've run this notebook, and you're going to get this kind of warning saying it's not offered by Google. That's totally expected. Don't worry about it. That's because I often don't want Google. I don't work for Google. So he prints it. I'm not sure why it's taking so long. Let's give it a sip and figure it out. There we go. Print HelloWorld. interactively, just like that. You can do simple one plus one things. Okay. I'm not going into too much time going over. I'm not gonna spend any time going over the basics, I'm afraid and if you got any questions, then pop them in the chat and we can get some if we can get to them. But for now, we'll try and get straight we'll try and dive straight into database. analysis. Okay. First things we're going to do, we're going to import some libraries. And we're going to pandas, which is a data analysis library. And pandas originally stood for something. I've heard of a few different things. I've read a few different things so far. And pandals, pandas dataframes series are the panels don't even exist anymore, so don't worry about it too much. matplotlib is useful for plotting. And, and from these both external libraries to base Python, and R, we've got collections, which is an internal Python module. And now we're just doing some, some Jupiter magic to make sure we can plot anything. And in default we expect so we'll go ahead and run this out, which will import pandas and import matplotlib and we're ready to import our data set. Okay. This is From from the pandas library, and the first thing we want to do is read the CSV. So I've already prepared a CSV and put it in a Google Storage bucket. So everyone should be able to access it. And if you just hopefully, if you just, if you just go to this link anyway, it'll download the CSV, but unless you really want to play with it in Excel, it's not necessary. But right now we've got when we run this self, we've got this data stored in memory. What this data set is, is the indices and multiple deprivation for for 2019. So again, the fine folks that opened data, Manchester know far more about their needs, the indices of multiple deprivation, I do. But it's a very important open data set. So I thought it's a it's a useful one to play along with. So what we've done here straightaway if we've printed out the And some of the information about the first few first few rows of data sight day, we've printed out every column of data and the different data types we can see kind of, and how many of these are no, which gives us some information about about how we expected. So if it turns out that it says 2000, which matches the total number of rows in this and the index. And I'm going to refer to data set as a data frame from now on. So just be wary of that. 30,000 rows in the data frame. And we know that in each of these columns, all of the entries in every single column are not null. And which is as we'd expect. And all the data sets you might have missing values and In which case, what hasn't knows, but that's another problem for today. So let's start let's start analysing that data set. Okay. And remember, we have any questions, pop them in the chat because And I've been doing this for quite a long time. So I might gloss over some really obvious stuff. And I apologise for that. And so we've got our we've got our ranking, we've got us a date. So okay, we know that the index multiple deprivation rank, this column appears. It's a rank ordering. Therefore, every single LSP in the entire country should have a rank one through 32,844. It's a ranking so presumably, every every single and so he, every single, one sing one single, should occur once. Now, I perhaps should have cleared the output of these cells before I run it. But as you can see here, when we run the windows command, we can see that some data, some of these rankings appear twice. Why is that you might ask? And that's a good question. I'm not and I couldn't say for certain But when we count the number of different occurrences using the counter objector. And we can set, we can find out that. And 32,700 of these ranks occur once. And only 26 of the ranks occurred twice. So there's two possibilities here. And watching that's not true, there's a bunch of possibilities, but two possibilities that I can think of as a data analyst. One, there's some problems with the data and 26 rules are entered incorrectly. That wouldn't be 26 might be 30 or something. Anyway, a bunch of rules entered incorrectly. Or it might be that actually, we've got some ranks that am exactly tied, which would make sense in if you got two and two ranks tied for the position that both go down as three and then they would occur and you will have the rank three occurring twice, which will explain what we've got here. So I'm going to break, take a step back here and explain what I've done. Because what feels like a small thing for me right now, could be a lot of different things happening if you if this is new to you. So first thing to do is I'm going to open a new code cell so we can write, and we can write Python. So we've got our data frame, we'll use a dot head command to look at the top five rows. Right like that. So we can see all the names of different columns. We want to see every single column of the data. So we can use a columns command, and we've got our nice little index. And this can sometimes be particularly useful when you've got say hundreds or thousands of columns in your data set because when you're when you want to have command, it's going to concatenate them off. But for the demo purpose, we want to we want to look at how many uniquely occurring how many times each value occurs. In this rank, so we're going to look at it's called the DS index or multiple deprivation rank column. And we can see the values here. So if we just compare what we can see here, what we can see that can see is the same thing, right? Except we got and kind of it cuts us off and takes us all the way to shows the bottom five and the top five and ignore the 30,000 in the middle. We can also see this useful little dot value counts command. And that's exactly where we can see what we've got what I've got going on here. The useful thing about the counter function within Python is that it counts the number of times something occurs. And yeah, it's a counterintuitive named command, but it comes in useful. So all we've done here is feed in and the result of this command into this counter function. So that we can see how many times different things occurs. Hopefully that's nice and simple. So on Like Step three, this shows that our expectation that each rank wouldn't occur exactly once. But we figured out how many times we've got observations that don't meet expectation. But Stan, but we've figured out how we can and perhaps explain that way. So as I was sitting thinking, what kind of important concepts can we introduce as part of data analysis, and I could not think of a more important approach than filtering. So, let's start out with a nice simple example of data analysis. In terms of filtering. We've got this we've got this data frame, right. We've got the City of London, backing a document all these places down south that we as over data Manchester do not care about. I think that's the official position. We want to we want to look at the Alexa ways They're out there monster based. Okay. So with this little puzzle in mind, we can, we can do that. So we're going to fill souls, and we only look at Manchester, Manchester local started district names. And just by running this command, we've got that. So let's, let's assign the results of this command to a to a variable. And we're going to copy it. And because if we make any changes to the master data, master data frame, we don't want those to be also to also change the base data frame. But that's, that's a bit complicated. So I'll come back to it in a moment, but I may come back to it later. Let's see. Let's see how it plays out. So even though like nothing's happened, we've done something so we can look at Manchester. I'm just gonna do Yeah. Just like that. We've got a different data. So this can this may seem trites. And certainly the example is quite, quite an issue, but it's a very powerful concept. That now we can, we've got our data frame just as much data. So if we want to perhaps see what the average multiple deprivation decile of Manchester is, we can we can do that. And we can compare how we would do that to the DataFrame of world. So how do we go about doing that we could then say, we take the column index, multiple deprivation decimals. Oops. One of the things with this is the keyboard shortcuts are different from my Jupiter notebook. So I'm afraid you have to bear with me when I make it better. So we've got 282 different LSI which is different from from the base data frame, which if we it should be 30,000 done, how many? There we go, we can see exactly what we're doing differently. So if want to then apply a function to it, like the mean we can use a built in and say okay, well the mean for the managers set of LS ways is 2.54 was the mean for the UK is 5.5. And by the UK, I mean every day every LS away in the entire data set without being filtered. And we could apply and we can apply an arbitrary function, and I'm going to use I'm going to use sub, I think this should work. Okay, that's a relief. So when I say Apply now to function here normally there are some built in functions interpret means pandas recognises by keyword. But also if we decide to write our own function and define it, we can still pass in here. So give me an example of that. Let's just define what's the best way of racing this. So, let's try and see if I can do this off the cuff about making a big mess of it. So we're gonna take in a series, which is the punters way of the pandas Name column. And we're going to return and this is definitely a quick get out. Like, don't pretend this is necessarily good Python code. Using the built in Python from the built in Python function for So, which I think it works like this, but you see some live data analysis and practices as I, as I think I remember what I'm doing, but I'm not sure I can. So hopefully, if we put in, we've now written our own function, which I'm really hoping works for. I think it should if we put in a panda series, and we're going to try out our own little Test series before. No, I won't try. I'll try it on the full base and the full data set. See if it works. What do you think it should do? Okay, that's a relief. I mean, I assume so Ryan's got no idea. But we what we've done here is we've created our own Python function. We've got a list comprehension of every element in the series. And we just create a list from it. We could in fact, do this a much smarter way, this keyword so that you all know what I'm talking about. And I'm not using Python shorthand, which do the exact the exact same thing. Okay, that's what we've. So now, if we wanted to, hopefully, unless I've done this in the wrong way, which I may have, I may have done this wrong way. So what's happened here is I made a mistake. But it's a useful mistake that will lead us on something later on in the notebook. And basically, what we've done here is we've taken the entire series in Thai kotlin. And we've we apply two functions the entire thing. And we're just which is generally I think, no, it was on the vectorization If we apply functions to entire column vectors or series of data, it's more efficient than applying and applying functions element wise. So what we've done here is we're applying a function to every element as we go through a function itself, which is similar to doing a follow through every element of the function. So, to come in to demonstrate that, I'll demonstrate it later on. Because I've actually written that I'm not gonna go off the hook again. So what was my point? So yeah, we can we can filter. Let's do some more filtering, in fact, and go from using a single filter where we just want to find every single LS away that's in Manchester and find and use to filter Same time, so every lso is in Manchester. And and every LS away in Manchester that has it desalle above eight. So as you can see here, we're just using simple operators. So we may ask Is this a string column DLS way, or authority district name is a string like Manchester, all the City of London. And which makes sure that they're equal. And because the index and multiple deprivation, I am d desalle, is greater or equal than AIDS. And yet, in our flow, numeric column, we're using the greater than or equal to operator. If we find that it's, it reduce the size of our data frame, even more. So if we just wanted to inform that. They'll show that we've just got the five entries and which, I guess is a reflection of the death styles within Manchester So we can, we can do a couple of things. First, we can change it so that it's to the way round, we can look at the decimals or lower the measure. And the key thing here to note with the syntax is we're using the, I suppose you'd call it dictionary operators and we're looking for the different keys. But if we if we just basically put our our filter conditions between brackets, we can add in some more so no facts. I'm not going to filter down to try another filter less. Let's go after health. I'm making my own worlds with again, scan only in well. Okay, so let's find an index multiple deprivation rank lower than 15,000. Let's get 15,000 and that's also in Manchester and has an impact index multiple deprivation IMD decimal lower than or equal to eight. Now, as I'm saying this because presumably the decimal the decimal should be evenly spaced, and 10,000 is above the 50%. So we should have the exact same number of rows should have 281 rows right let's let's try that out and see what happens when you say greater than or equal to 10,000. So you know under this mistake command, forgot to change the operator. I think this mistake I made Let's hope that's a mistake. So as we got different filtering conditions here, and we need to make sure we chained together chaining is patented, the right name, I think is the right thing. doing this a long time and some of the knowledge is just up there are apparently may not be available, but feels like it should be up there. And except apparently gone from 261 to 69 rows. So I suppose this shows So yeah, that's that's not what I was expecting. That's not what I was expecting at all. So let's, I'm just gonna, for my own curiosity, try figure out what happened there. So we're going to comment out this where we got it. We got 181 rolls. And the desks are right, sorry, the desks aisles are lower than eight. So yeah, that would make sense. There are fewer, I think. I'm thinking myself in circles. But anyway, the point is, we've demonstrated how to do multiple filters, and that's important. Come back to my earlier points about Sam functions to different roles in different elements of the data frame. While we can deploy apply function to entire, an entire series, as we've done here, and although perhaps not the most elegant way, it's still something we did it. And we can apply functions to each individual elements and in a column or a series. So as you can see here, and we've got a capital M, but lowercase the rest of word here, and we'd like to make everything uppercase. So what we've done is we've applied and we're only doing this to a particular column, okay, so let's Yeah, so we were using the dot apply keyword, and we're using the string author. So just show you how to string darker work keyword work. So let's There's no argument but it means no one which makes perfect sense for we put in Hello Manchester it should capsulize how much it can also work in a slightly different way. So, and this is kind of some of the nuances of how Python works. So str is known as what is what's known as a type. And actually, yeah, it's what's known as a type. And we've got string hurt, which as you might guess, is of type string nd m, you can find out what type any any Python object is by using the type keyword. So we've got a Hello monster string, and we can use the author method on it. And it makes it uppercase. So what we're doing here is for every single element In the entire data frame, that's right. That's not true for every single element in this column in the entire data frame, we're applying the string dot upper function on it, which will make things that upper case, as you can see, should look. And because we assigned the outcome and the output of this function to, to this keyword, which is local authority, district name, we've overwritten and what the data frame was before. So. So before when we first brought in the data frame, and if you if you noticed, and the execution orders increments. So when I was going through this before, and I was messing about with filtering, we're on number 34. And execution number, so it's also every time I want to sell it'll increment. Now we've got 39. So we're almost looking back in time there. So we got things monstery lowercase, but now after we've run the function, and we look at the output here DataFrame we've got the City of London in capital M in capitals letters. Okay, so I know I've been a little bit all over the place. I've got a lot of different things. And do you have any questions at the moment? I suppose I'd say Simon would notice us, James. So not sure I can see the chat window. Yeah, there's there's no questions in the chat at the moment, john. But if anybody does have one, please, please do drop it in the chat box. But again, there'll be there'll be a chance at the end as well. Okay, right. So is this. We're good to continue. If you've got no questions, and I'm just going to go ahead and assume that we were all perfectly I'm perfectly fine with what I'm saying. I've just been boring. Yeah. So I apologise. So one of the key things and I guess some of the key things in in data analysis is the idea was split apply combined. And I'm not I say Hadley Wickham invented it. And he wrote a really, really useful paper on, which is kind of how the concept of a data frame came about. And if you use arc, you'll be familiar with Hadley Wickham, and affiliates familiar with data frames. So the idea behind the split apply combined strategy is really simple. And, and it's in the name. So you've got your data frame. So up here, when we first load it in, there's nothing special about it. But we can, rather than trying to process the entire data, data frame in one go, and it can be more efficient, or perhaps it can be more useful to actually solve the problem of doing to break things into different into different groups. So for example, we might split by the local authority district name so we could think treats and we can treat at Manchester, London barking all separately when we try and use different aggregations are not necessarily just variations, but all the functions just on these groups. So I'm going to show you what I mean. So here's where we are going to figure out exactly what the mean of each local authority district code is. So we're using the code rather than the name, but we can, we can still use the name. So, hey, we've got the data frame objects, and we use the group by method. We're using the column local authority district code. And then we're using the aggregation index and multiple deprivation. And we use a mean on this column. Okay, for one, and hopefully that should work fine. We can change it up so we can use other column names. So let's try and use a city name. That's probably a reason why I use the district code rather than the city name when I first wrote this term what it was. So, we can see alladale Amba Valley there then mean rank, we can do more than that. In fact, we can we can figure we can have a look at what that and look at some, shall we, I mean, not that this will mean anything semantically, I'm afraid, but we can still still do it. So, you can see already we've broken down this into doing multiple aggregations on the same on the same column based on where somebody is all based on that LS based on their city name or based on the LSI. So, it's really quite powerful. So we try and break it down on how this works. And we can We can break it down by crew. Right? So. So if we do group by and we and we can get two things out, we get the name of the, and the name of that group and the group itself as a data frame. So under the hood, this is kind of how it's working, albeit much more efficiently. We'll get the group and then we can apply the kind of, we can get the group we can get the colour which was I think it was I think tank. And then we can do the sum function. I'll give you the main function. But the really useful thing about this is, we all do it kind of in one go. And in one nice little thing, and we can chain more more options on the hair. So perhaps we'd want to Reset the index. And let's make this a little bit simpler. Let's test the mean. And then let's size. Taking a step back. We don't just have to agree it by one thing, we can aggregate by multiple things. Let's see if there's any multiple things we can aggregate in here. So we've got LS LS, we've got local authority district coach. It's not gonna work. The problem with this dataset, while is extremely informative for actually doing stuff, it's not a great example data. Okay, let's So for every local authority district meeting, we can break down and see where we can break down into exactly what each. It's a different deciles in each local county district. So let's kind of show how it works. So if you're, if you're not familiar with Python, this may not make a huge amount of sense, but this is just a simple following. So the output of df dot group by is, is a five iterator, which without boring Yeah, it's basically like a list. And but it comes out with to like a tuple of items. What the top level objects one is a name, which is a simple string, one is a group which the data frame our data set. So the poles that so when we break gotten through one iteration, and then we stopped. So we've got other two, I'm sorry, I'm probably mispronouncing either. If there's anyone here from other I apologise. And we look we've got only and in this in this particular, this particular group will only have an LS away is from others that are in the IMD desalle t. So, we'll have a look at this. pretty small as you might imagine, but I'm just disparaging I don't I don't know how many people there I don't know how big it is. But I imagine it's pretty small. And so there's only one one away with an IMDb desolate. So that's what you get. But we could we could do something like something get and we're going to make absolutely no sense when there's only one element, but let's do it anyway. humour me humour me. Actually. No, not the decibel. But something indexable for decoration. could do so do mean this does that sound about right? Sure. So if you want to do this for everything and nice efficient ways see tries to code by hand and after coffee get off the hook again. You can aggregate and index both operation are we going to use to me again? So we should figure out we should be able to see exactly what the mean and IMDb rank is for. And it unfortunately is going to mean like nothing because what's the mean of a decimal but it's the data set we've got starting the day, so we've got what am I doing? So generally the aggregation function, and it can take a dictionary of operations. So a dictionary, the columns and the different operations you want to do with them at the moment is quite simple. Because it's a data set using but that's kind of how it goes. So, here we can see either the index multiple deprivation and the mean of the index multiple deprivation. decile two is 4639. The mean for decile three means decile four, and we can see how it goes to every single group. Okay, so I've given some pretty trite examples of how this, how this works so far. Hopefully that makes sense. I'm gonna pause for a moment while going state and the next notebook, okay. And if you're following along, this is, I think how you do it. So there's a couple of questions here. I wonder if now's the time to Yes, yes. The ideal So first question is what is the backslash for in the split apply combined is it's to chain the commands. That's a very, very good question. I apologise for glossing over it. So where are we here? Right this right now. It's just a convenience thing. And so if we want it without the chain, they'll still one. But it's just if you keep chaining these things more and more, it takes ages. So it's just a way of kind of saying, I'm done with this line. I'm gonna go, I'm going to continue on the next line. Great. And another good question. It says like these functions seem like set based functions. Wouldn't it be better to load data to database and use SQL? Yes. But for the purposes of this demonstration, right, yes. So that's, that's a really good point. And if you're doing simple set based operations and when you are trying to And you're doing built in SQL logic, then yeah, it can be more efficient. I mean, obviously, it depends if you already got an SQL set up. But yeah, it usually if it's already in SQL, it can be more efficient to do that. And however often you don't have it in SQL, or you want to use functions, all types of logic that aren't that aren't included in SQL, or that isn't set based operations. So. So in those kind of situations, yeah, you wouldn't want to be doing that. Right. And, but no, I mean, if you do no SQL, you do. group by command is is found in there. And it's not a coincidence that they've got the same name. Correct. Yeah. That's it for the questions. Thanks, john. Cool. So we go back to file. Okay. Just in case one. following along file, upload notebook, GitHub type in Jasper jgr and the repository and the same repository before. We'll have a look at plotting, just see how plotting works. That should be fine. Excuse me. Okay, so you've seen similar data sets before. That's right, similar Python imports as before. And we've imported Seabourn which is a visualisation library. But also it can bring in some and we can use it to import some, some artificial data sets. So we can have wellness and we got sales at DataFrame that we're all extremely familiar with. And from Yeah, everyone seems to be familiar with pandas before and it's very few questions. So, the aim of this notebook is try and just do some simple visualisations show a little bit of what's possible. So, kind of the most simple plot, I guess, and Elise most commonly seen is a scatterplot. So and this is all built into pandas already. We've, we want to set the table and see the total bill. Okay, so I'll take a moment just so you can familiarise. familiarise yourselves with data set. It's all about the tips dataset. I guess we're in what I assume is a some kind of cafe. I have no idea. Yes, tell us about the tip, sex smell cut dinner date. Time sighs and please don't take any information from this data set as being representative of anything in the world because I don't think it's, I don't think it's a real data set at all. But it's there for purposes of having some data to to look up. So the correlation between total bill size and the tip is, from this, it looks pretty, pretty similar as you might imagine. And in that, as the bill gets higher, the tip also gets higher. We could I mean, one things you might want to do is to draw a line of best fit to to do this but that's a little bit beyond this doctrine. And see Barnett does have does have some useful functions. So maybe we'll get into that later on if we've got time. because everything's built into because the plotting functions are built into the the data frame object itself. We can just pass the column names as the x&y parameters, we can past the visualisation of the plot, the plot kind of scatter and title and whatnot, we've got ourselves a decent enough plot ready to go, which is really useful if you're just doing some data analysis. If you're looking to present it someone you might want to polish it a little bit prettier. But it's as simple as that. Again, we've got a bar plot now. Sequel can't do this can't do this, we can group by day by we can group by date and do aggregations of some sort, we can just see if that's all I'll show you how this is working on a kind of a line, an argument by argument basis. So we've got a df AI data frame here. Just seen it before, right? So we want to plot tips on a day by day basis, sorry, total bill by day by day basis. So quick buy, as we showed before. Simple enough, right? So we've got ourselves a group by objects. If we go and set apply, we want to apply an aggregation and the aggregation is gonna be some. So you can sum all of the other columns what they like. And so on Thursday the total bill was just on disk disk cafe made just over 1000 pounds. And it made 171 pounds. I say two pounds. I have no idea Yes, I was crushing pounds 131 pounds of tips and size is 150. And the size, I think yes size I assume from this is the total number of I think covers is the technical term people who died on the day. That's what I'm assuming anyway. So can then be set index and reset index then as you can see Hopefully that was clear. And when we do the group by because we grouped by day, the index changes to day, as we reset index day becomes just a regular column. It doesn't change anything else about the data frame. But it's a useful thing to know. So on on another little tangent. And we will say for example, while we as we do this aggregation today is the is the index, say we want to look up different elements from that index. So let's only look up the values from Friday right. Just like that, we've got ourselves a series of objects and which shows the values and the VAT values just on a Friday. Which should be what we're going to show on the on this PowerPoint in a moment. So But we're going to reset the index, which comes back to where I'm at. Yeah. So until that date changing from index for regular column. What this allows us to do is when we use a plot method, we can set day as you can set the x as the day value, and set by total bill, which we've got right here, and apply as a battler. And as simple as that, we've got it. And if we wanted to, we could change the other plots to tip. And you can see how it changed again, and, and we can change again to size. I think if we sent it out, without why it should show them all at once, but I can't remember that's true. I did. And pleased with that. So yeah, well, we've just by simply doing that, we could it's a very powerful plotting function for a bunch of different things. Similarly, we can do a histogram straightaway. We could do a histogram that's built in, in the same way we could do anything else. We don't need to do any aggregations on this because it works on the on the base data. But again, we can show you tips. Tip isn't it. So we got a history and a tip right there. And it's all just on the name of the column. There's a lot of the parameters we can and we can try that I'll deviate to where to play with these when you call me if you want to go through it yourself. So if we take a step back, and we figure out how is how is pandas doing this and why there? Why did I bother talking about matplotlib at all, and why don't bother with the funny cell magic at the top of the percentage point Because all the plotting in pandas is built on top of matplotlib matplotlib, if you're familiar with, I guess old fashioned scientific computing and come from MATLAB, and so, when this when the matplotlib was originally started, an awful lot of scientific analysis was done using MATLAB, MATLAB and MATLAB was known as having particularly good graphics, graphical visualisations that kind of stuff. So people started to try and put the same functionality into into Python. What we've got now is a bit of a is it's a bit of a legacy code base. And because this matplotlib I think it was, I think it was 2009. But it may it may have been even earlier than that, and people started using that popular start developing it. But it started off with an API it was it was meant to be sit and reflect that map. Matlab is We've gotten long and it's gaining maturity. And it's more useful and more ease use more in Python is older people moving away from MATLAB. And it's got its own kind of pythonic API. So that can get a little bit confusing. But if you want to do simple things, and it can be really it can be really quite easy. So what we've got here is we've got two different ad plots so and just by using this PLT subplots command, we can show a diff a bunch of different it's never worked expected. So because we've got with this PLC that subplots we can show a bunch of different figures are not figures. We're showing one figure, but we're showing multiple different and plots on the same figure. So we've got one row and two columns. Got two rows and two columns, two rows and eight columns. You can go crazy with plots are not going to plot that yet. But simply here this is this is how easy it is to get the pandas objects to interact with the matplotlib. There is a stick keyword within the plot function within the plot method. And to to demonstrate to specify exactly which plot wants to plot on. So, if we don't, it'll just create so oops. So we've got these two plots that we've created here, x one x two, and it creates its own plot here for this one here, just knowing that was pretty so that's, that's a very basic introduction to matplotlib. And we can do some Much fancier things within matplotlib, so we've got this weird p plot as well. So yeah, it's like plot could do autocorrelation plots. That's a huge amount of stuff in there. But for the moment, that's all I. That's all I had prepared to have any questions, any particular types of aggregations on that data? So I'll do a data set. I'll fancy things people will be curious today or anything like that. Hopefully that said, not too overwhelming. good introduction. 

Transcribed by https://otter.ai
