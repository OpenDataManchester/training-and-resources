Diarmuid McDonnel  
Yep, fantastic. I am. Hi, everyone, thank you for giving up a portion of your evening. It's quite a sunny evening up in Glasgow, and I'm imagining it as in England as well. So thank you very much. I'm going to do probably 25 to 30 minutes of a web scraping demonstration. And we'll use two different examples, we'll use a real website, but one that has been tailored for web scraping. And another one, which is from one of my research areas, which is charity data. So it's another form of open data. That's really valuable to me in my research, I'm just going to post a link in the chat, because you can follow along as I'm demonstrating what I'm doing. And you don't necessarily have to, it's just if you want to if you have two screens or you want to split your screen, and click on that link, and it'll launch the programming script that I'm going to use right So I'm about to share my screen. If it doesn't look particularly well sized then please, some you let me know. And yeah, we're getting the whole screen at the moment. So we're getting quite long and yeah, that's totally fine. I'll enter in slideshow mode, but as long as you can see it. So very briefly, um, Julie and I are working on a new project at the UK data service, which is looking at new forms of data. And as Julia probably outlined, we tend to store and reuse large scale social surveys, census data, and a whole mix of different social science datasets. And we're now focusing on generating new forms of data. So using web scraping techniques, and looking at API's online databases, social media platforms, text and image data, and etc. So this is a bit of work that we've been doing on collecting data from webpages. So you've clicked on the link, it should take a few seconds to launch. And they should basically you should see something very similar to what I have here. So I'm just going to enter slideshow mode. And does that look okay? You're just on mute.

Sam  
I was on mute. Yeah, yes, it Yes. It looks fine to me. 

Diarmuid McDonnel  
Yeah. Okay, so don't keep it sizing. 

Sam  
I was gonna say if it doesn't look fine to anyone else, please feel free to mention in chat and I'll speak up. 

Diarmuid McDonnel  
Absolutely, because I can move it across to the different screen. Yep. So today, we're going to take a look at how we can use Python to collect data and found on websites. And we're just going to approach a couple of problem solving exercises getting you thinking in computational terms about how you approach a data collection issue. So we'll do about 30 minutes. It says a bit longer if you're working through the document yourself, you know, It's a self directed learning material. And I'm not going to assume any knowledge of web scraping or Python and whatsoever. So I'll take us right the way through from first steps. And you're all probably from different disciplinary backgrounds, I'm going to use some social data, but the techniques broadly apply to any data stored on a webpage. And so what I'll do in this lesson or demonstration is outline the kind of very broad, general steps that you have to undertake to collect data from a web page. And then we look specifically at how Python is particularly good for requesting data, parsing it so making sense of that and extracting the information you need and then saving it to a file and for future use. So if you're working through this document yourself, it's called a Jupiter notebook. I'm not sure if people are familiar. And it's an electronic notebook, like something you would write in, you know, in your paper notebook. It's a bit better you can make In live code, the results of code and as you can see, as well, you can write text explaining the code and the output that's being produced. So to give you an idea of how it works, here's a little very simple bit of Python code. And when I press Shift and enter, or Ctrl, N, enter on my keyboard, and it asks me, you know, for my name, when I supply it, you know, it gives me a nice little interesting and slightly childish message about enjoying what we're about to undertake. So what is web scraping? Well, it's a computational method for collecting data stored on a webpage. Computational is the key word, because I'm pretty sure we can all think of scenarios where we've copy and paste the text from a webpage and saved it into a txt file or an Excel file, for example. And you know, or we've right clicked on an image and we've said Save as picture and we've downloaded something from a webpage. So really, it's we're trying to do this computation. We're trying to use programming code. So we can be more accurate, more efficient, you know, we can be quicker at collecting data and stored on webpages. So what's the general approach? So basically, there's six elements. Two of those elements are things you need to know about collecting data. And there are four elements, which are tasks you need to undertake to collect the data and that you're interested in. So the first thing you need to know is if there's a web page of interest, and you need to know where that web page is located. So on the web, a web page or a website is located at a web address, or a URL or a link, you'll have heard these interchangeable terms. So for example, at the UK data service, and you can access our home page and using this link here. And how you establish that is up to you, you'll have an idea of what webpage you're interested in. And you know, a search engine search will provide the URL or the address that you need. So you know, you've identified a web page. And the next step is to find the information you're interested in, on that web page, itself. So that's a visual manual inspection, you open the webpage in your browser, you scroll through this, you think, Okay, I need a paragraph that is roughly halfway down the web page, for example. And it takes a little bit more than that we need to dig into the source code of the web page. But the first step is just simply saying, where's the information? Where's the link? Where's the paragraph? Where's the table? I'm interested in collecting the data from. So we have our two key pieces of information. Then we get into the interesting bit of actually collecting the data. So the first thing we need to do is request the webpage itself. So this is akin to opening your web browser and copy and pasting the link in and then viewing the webpage that is returned. We can do all that within Python, which is quite interesting. Once we get the web page back, we need to make sense of it, we need to tell Python that, hey, you're working with a web page. Therefore, we can start extracting elements from that web page quite easily. The penultimate thing is actually collecting the information we're interested in. So finding the particular elements of the webpage that we're interested in and extracting the information from them. And of course, we want to, you know, save the results of what we've done to a file that can be used much later on. So these six kind of general elements of collecting data from a webpage, and we can think of this as pseudocode. So if I tell Python, hey, request the web page using the web address, it clearly won't understand that and I need to translate it into Python code that actually works. And but these six steps will underpin the Python code we're about to write so it can be very good to write out the general steps needed to tackle a coding problem.

Diarmuid McDonnel  
So let's start with our simple example, as I'm going to call it. So we work through the six steps that I just outlined using a real web page, but one that has been designed and for practising your web scraping skills. So of course, the first stage is knowing where the webpage is located. So it's got a slightly indecipherable, web address or URL. And if I click on it here, and you can probably see how it looks ugly, quite small. It's a very simple web page. And it's basically an extract from Herman Melville's novel, Moby Dick. And our task will be to extract that paragraph from the webpage. So again, you can also use Python to view that web page. So as you saw, in my web browser, I clicked on the link and it opens it up, and Python can be used For the very same task, which is quite cool. So I can actually embed that webpage in the Jupiter notebook that I'm using. And that just kind of keeps your workflow, you know, nice and tight and keeps it all within the same programming environment. But the choice is yours, you can, you know, right click on the link and open it up in a web browser, or you can use Python itself and take a look at the webpage. So what am I interested in? So where can I find the information that I want? So I need to extract the text from that web page. I don't want the heading. I just want the paragraph starting from here all the way down to here. So I've outlined the general task that I want to do. websites are made up of a programming language called HTML. You might have seen this HTML basically Describe describes the structure of a web page. And a web page has a number of elements. So there's a paragraph, there's a table, there's an image, there's a video, there's headers, there's various different elements. And then each element is uniquely identified by something called a tag. So a paragraph is identified by a p tag table by a table tag, and a top level header by the h1 tag, for example. So HTML usually gets made fun of for being an incredibly simple programming language. But that really works in our favour and for web scraping. And so we visually inspect to the web page to see what elements we want to extract. So I know there's a paragraph near the top of the Moby Dick web page that I want to extract. It's not as simple as that. Basically, I need to know where that paragraph exists in the underlying HTML of the webpage. So to do that, again, this is a manual process and some of you might have On this before, you'll have right clicked on a web page, and you've chosen something like view page source, for example. So I'm using Mozilla Firefox at the moment. So if I go back to the web page, if I right click view page source might be a bit small. But you can see here, here's the underlying and HTML that underpins this webpage. And now that I'm looking at the source code, what I can do then is start to identify the tag that I'm interested in and scraping. So you can see here, because I'm interested in a paragraph, that there's a p tag. So there's a set of opening and closing p tags. Within that p tag. We can see here is the paragraph I'm interested in, scraping. So again, this is a really simple example. Thankfully, what I need to do is I need to scrape this page I need to find the p tag. I need to extract the text contained within the p tag. And here's just how the HTML looks in the Jupiter notebook. So I've just copied and pasted it into the notebook just to make it a bit easier for you to see. And, again, clearly, we want this information here. And we want to ignore the header. For example, we don't want to scrape the header.

Diarmuid McDonnel  
So we have the necessary information. We know the web address or the URL that we want to scrape data from. We know the element of the webpage that we want to actually collect and extract. Let's get to the nitty gritty of actually doing this through Python. So there's a slight preliminary step when you use Python, and it comes with lots of inbuilt functionality. You can do lots of calculations, you can do some things. You can print messages to screens, you can work with your file system. I'm using Python straightaway. But for most computational tasks, most data science tasks, computational social science, etc, you're going to have to bring in additional functionality into your Python session. So we do this by importing modules. And maybe if you're familiar with Python already, or you've used the our programming language, you know, you'll be familiar with this, you'll be very used to doing this. It's a very simple task. And you basically just tell Python, import the module that I need for web scraping. These are the two key modules that you need. One is called requests, and as the name suggests, is for requesting webpages and then slightly oddly named module called Beautiful Soup, that we're just going to rename a soup while we're doing this task. And that's for telling Python to recognise the tags in a web page. Otherwise, when you request a web page, Python basically treats it as a big blob of text, it doesn't know that a p tag means that it's a paragraph, etc. That's why you need the beautifulsoup module to tell Python, we're working with an HTML webpage. And I'm just going to bring in an additional module, which is good for working with the operating system of your machine. So if I want to create directories or folders, I can use the OS module. And then I'm going to run this and if I import the module successfully, it's just going to print a very simple message to my screen. And yes, so thankfully, no errors with that. And it's successfully imported the modules. And just in case you think I'm running through something that I've designed to work, if you're actually working with the link I sent you, you'll see that this is all very much live very much connecting through the internet to the webpage of interest. So potential for errors are Right. So now we want to request the webpage that we're interested in. So the first thing I do is I define a variable that I've just called URL. And you can call the variable what you want web address, web add, link, you know, you can call it whatever you want. I've just decided to call it URL. And this stores the web address of the web page that I'm interested in requesting. So next, I use the requests module. And then I use the get method from that module. And and I get the URL of the webpage I'm interested in. And I just have a small option. You'll have seen this before requesting webpages. If the web address changes, then sometimes it can redirect you to the correct and web address. I've just specified that option just in case that happens. To be honest, we can take it out, and this should still work. So I get the web page and then I want to store the results of that request in another variable called response. And this is just kind of standard variable naming practice. And then finally, I want to check if that request actually worked. The way to do that is there's a status code attribute for my response variable. A status code of 200 means well done, you successfully request the webpage. You'll have seen a status code of four or four, five or one, for example, if you've requested web pages before. And if it's in the four hundreds, that means you've done something wrong with your request, you've made an error somewhere in your code. And maybe you're not connected to the internet, for example. And if you get something in the five hundreds, that basically means there's something wrong with the website. So maybe it's down for maintenance, maybe it's under attack, for example. And there's different status codes but you're looking for 200 means you've been successful and 405 hundred means you haven't Okay, so all I've asked Python to tell me is whether it's been successful or not. And it's giving me 200, which is really good. So I've requested the webpage. You're probably wondering what the hell have I requested because when you type it into a web browser, it shows you the web page. And as you can see, Python hasn't

Diarmuid McDonnel  
done anything with the web page just yet. So there's two things we can do. And we can look at the content of the web page. But a first initial step is just to kind of look at the metadata behind the request that I've made. So there's a headers attribute associated with the request. And if I take a look at it, I get some metadata about what I've just done. And so you can see the date that I made the request and you can see it's a it's a GMT and we're at BST, so that's why it's an hour behind. So I request the webpage on this date at this time. What are they actually requested, and it's the HD DML file. And we can see other examples later, we can talk about it where the content type will be a PDF, for example. So you've downloaded a file, and different different content types as well. A lot of the rest of it is not terribly interesting unless you're a web developer, so we can skip over it. But it's just good to know that there's metadata behind and what you've done. You know, as a researcher, I need to provide a breadcrumb trail and to any analysis or result that I published. So it is good to say I downloaded the data on this way on this same date, for example. And that allows people to kind of check what I've done. So as I've said, What have I requested, I haven't shown you any results just yet. So with my response variable, there's a text attribute which stores all the text on the web page. And so we can take a look at that attribute just now. When I execute that command, As you can see, the Python shows me all of the text on the webpage. And as you can see, it's the HTML. So it's not the actual text that's shown on the webpage. It's the entire HTML. It's all of the source code underpinning this web page, and is what Python has requested for us. And that's where the beautifulsoup module comes in. Because we need to now make sense of the fact that this is the HTML page, and that we need to pick out tags and that we're interested in. So we call this task parsing the webpage and are basically understanding the structure of the webpage. So we've got the text, it's stored in a variable called response in an attribute called text. And we need to pass this into the beautifulsoup module, and then that'll make sense of the mass of text. So I use a method called soup. Which I defined earlier. And I take the text of the web page, I treat it as HTML. And then I store it in a variable called soup underscore, and response. And now you can see we get a similar result that we did before. But now we're starting to see the Python kind of understands that it's a hierarchical structure. And we saw that ourselves when we looked at the source code. Let's scroll back a bit here. Yeah, so you can see when you actually look at the source code, you know, in your web browser, it recognises the fact that it is a hierarchical structure. And that's what helps us find and the tags that we need. So it's important to understand that in a web page, something that's above something else actually matters when you're trying to scrape it. So now we've taken the text, we've parsed it or treated it as a Beautiful Soup object in Python. And now Python clearly understands that we're working with HTML webpage. So now we get to the actual interesting and fun bit, which is actually taking the information out of the webpage that we're interested in. So we can use Python to navigate through the web page and find that the tags that we're interested in. So we know that we're looking for a paragraph, a paragraph is identified by a p tag. Very simple. We use the find method and on our Beautiful Soup variable, and we want to find a p tag, very simple. So find the p tag, store the results in a variable called paragraph and give me a quick look at the paragraph. Now you can see we've kind of, you know, filter down the entire web page, just to the element we're interested in. So now you can see there's no header, there's no header, there's no HTML tag. Now we've found an extract Did the paragraph and of interest. And the final very, very simple task is just to tell Python, ignore the P tags, and give me everything else and contained within them.

Diarmuid McDonnel  
We're near the end of the scrape, maybe you're thinking this is probably a little bit easier than it really is in practice. So we just need to extract the text from within those p tags. And that's very simple. The P tags have a text attribute, because it's text that's contained within them. All I'm saying is for the variable i created called paragraph, give me the text that's contained within the P tags, store that in a variable called data and just show me the variable. And now you can see we've whittled it down. So we started with all of the source code, we found an element within the page we're interested in. And for that element elements, we've extracted the text and within it, so now we're done effectively, we With this very simple web scrape, the final task is just to save the results to a file, I'm going to do this really simply, I'm just going to create a txt file. And I'm just going to call it very obviously, Moby Dick scraped data and store that in a variable called out file with the file open in Race mode, referring to F for shorthand. Write the data to the file. And it's as simple as that. So that worked. I didn't get any error message. But of course, I need to check two things. I need to check that the file was actually created, and I need to check that the actual data was written to the file. So does the file exists? Well, I can ask Python to list all the files in my current working directory. Thankfully, yes, you can see that there is a txt file and located in my current working directory doesn't actually contain anything of interest that actually, you know, did the data actually save to it, I just do the reverse of what I did previously, with the file open in read mode, read in its contents into Python. And take a look at the results.

Diarmuid McDonnel  
And just, you know, prove myself again, and I did create a variable called Data previously. And, you know, let's create a new variable just to show I'm not calling the old one. There you go. So another variable called new data, it stores the contents of the file have just opened. And here we are in Python, again, we can start working with the txt file. So maybe you want to do some keyword searches. And maybe you want to do text mining, natural language processing, we can, we can move into the very advanced kind of analytical techniques for whatever purpose, but it all begins with getting the data and storing it and appropriately So that's a very simple example. What does it actually look like when we want, you know, resource research relevant data or rich data, or a webpage that has lots of different things going on. Maybe it has multiple paragraph tags, and we only want one in particular, for example. So I'll take you through a very quick real example, you know, that I need for my everyday research. So it gets more complicated in the real world. And, you know, you may be interested in data that's spread across a webpage. So maybe there's multiple paragraphs, and you need all of them, for example, or maybe there's information that's spread across multiple web pages. And you'll have come across that and trying to think of an example but think of any web page where you get to the end of your scrolling. And then it says go on to page two, for example, excuse me, you know, to keep viewing the results. How do you actually loop over all the pages and extract the data you Need, there may be multiple paragraph tags, you only want one. And so how do you filter down through those? Yeah, many other potential issues. Maybe a website actually blocks you from web scraping, maybe it identifies your IP address and, you know, bans you from doing it. And it gets pretty dark and pretty complicated pretty quickly. But we won't go We won't go there tonight. And so we're going to look at a quick social data example and from the real world so for me personally, I research and charities across the world. And so the UK for example, Australia, New Zealand, all have different charity sectors doing similar things with similar data. And but I need computational methods to collect that data. And some of it is made openly available. Some regulators actually you know, share a CSV files and Excel files, and but actually the best source of data on charities tends to be the website of the regulator, which is designed for you know, you and I to just scroll through and say, I wonder what you know, Oxfam made last year. I can search for Oxfam and I can view its results. And as a researcher, I'd really want that data, you know, for myself, and I want it for the other 160,000 charities in England and Wales, for example. So let's go about trying to scrape data relating to policies held by charities. Maybe that's not terribly interesting. But if you think of everything that's happened in the charity sector in the UK recently, and you know, safeguarding vulnerable adults and children have been harmed in multiple countries. And it would be pretty good to know if those charities actually had a safeguarding policy, for example. There's lots in the news about, you know, data breaches, and maybe it'd be good to know if a charity had a personal data policy or just a deeper policy. In general, so hopefully that's interesting enough for you, it certainly is for me. So let's work through our steps again. And I won't spend much time on this. But we just import the modules we need, you can see that basically, the requests, the Beautiful Soup, and the OS modules, I'm going to bring in a couple of more. The important one is called pandas, which is for working with data frames or datasets in Python. And this is the same task again, I just need to bring the modules into Python. So I can do my data collection. So the web address that I'm interested in, as I said, there is a charity regulator in the UK. And actually there's multiple there's one in Scotland, one for England and Wales, one for Northern Ireland, and of course, one for the Republic of Ireland. And in case you ever think the UK is a homogenous place, you know, we're really split when it comes to charities even let's keep it simple. Let's just try and get policy information for Oxfam. So Oxfam has a web page here. And you know, if you want to know how much income it made in its last financial year, you know, a breakdown of its income, if you want its annual accounts.

Diarmuid McDonnel  
If you want to know who sits on the board, lots of really interesting publicly available information about charities, for us today. There's a little list here, and there's a policies header. And then there's a list of policies that that charity at least says that it possesses. Maybe it does, maybe it doesn't, but this is how it presents itself and publicly, to the world. And again, we can use Python if we want to actually look at that web page within Python itself to save us, you know, going out into different tabs. But as you can see, it's the exact same thing. Great, so we know, the webpage and we want as I just showed You, there's a Documents tab, there's a heading called policies and under the heading is the information. And I want to scrape. What does that look like in terms of the source code, and it looks like this. So there's a div tag, which basically means divider or section. So that's a section tag. There's a header three tag. So that's the heading policies. And then there are these multiple span tags. So it's not a paragraph. This time, it's a span tag. But within the span tags is the text we want to extract.

Diarmuid McDonnel  
So it's not terribly difficult, but it takes a little bit of extra work than the simple example. So we go back to basics, we request the web page, we load it into Python, and then we just check our status code again, so we get 200. That's good. I've successfully requested the webpage. Again, we tell Python to treat it as Beautiful Soup object. And so again, use the soup methods on the txt or the text attribute and treat it as HTML. In this instance, I haven't viewed the HTML code, but actually, we can, but it'll just look. Yeah. So obviously, it's a more complicated webpage. So there's so much more source code, and NF, but if we were to scroll through it, we will eventually see the list of policies that we're interested in. But there's quite a lot, so I'm just gonna hide it. So basically, I've requested the web page, I've treated that as an HTML file. The first thing I need to do is well, just as a quick example, let's say I wanted to find all of the URLs or links on that web page. And instead of using the find method, I use the five All method because I'm assuming there's going to be lots of links. And, you know, one that webpage, I mean, we've got five here, here's the link to 2018, finances, 2019, and etc. So just as a simple demonstration, and find, find all the links on that webpage. And you can see Python returns a list. So there's more than one a tag that identifies a URL. And is there anything really, you know, interesting, a lot of them are internal links to the charity regulators web web page. And here you can see there's a link to some of the images that are stored on the web page. This is an interesting one. So this is the actual link to the PDF and for its annual accounts. So if we copy and paste that, for example, and that should download the PDF, so you can see how searching for links on a web page in the same way that Google's crawler, you know, that's how search engines work. They search web pages for other web pages, and you know, Link All those together. And yeah, so that's just a really quick example. We're not interested in the links for our purpose. We're interested in the list of policies. So we know what the source code looks like. There's a div tag, and within that there's multiple span tags that we want to extract the information from. So basically, we want to find all of the div tags. So basically, the div tags on that webpage, and all share and quote unquote, a unique identifier. So some of the big div tags and can be identified using this. Identify, identify, you're here. That name doesn't really mean anything in particular, it's just what the regulator uses for the webpage. So basically, find me all of those sections that have that ID, and then tell me how many of those sections and exist as you can see them There are seven of those sections. So now our task becomes which section contains the policies. So this is how it gets a bit more complicated. And in a real example, and I want to explain all of the Python code here, and basically to help me narrow down to the section I need. And I know that the section I need has a header called policies. So basically, for every section in the list, and that was returned to me, and look at each section, see if it contains policies, and if it does take that section, and out when I run that code, and you can see that the section I want is in position five, so it's the fifth section in the list. And then I basically just extracted that section from the list. So now we're after taking the source code, we're after finding all the sections that contain section we're interested in And then we've actually filtered out all the extraneous sections, and we're left with one that we're interested in. So now our task becomes similar to before, for each of those span tags, take out the text contained between and save this to a file. So again, just as I've said, and for every tag in the policy section, so find all the span tags and take out the text from within them. There's just a little bit more going on to actually construct this

Diarmuid McDonnel  
and as a data set, but the result of it here is a variable called policy list. And as you can see, it's a list of observations. So the charity name, policy, one charity name policy to et cetera. And so I got rid of the webpage but just to do a manual check that I have actually collected the policies correctly. Okay. Has so Policies risk management is first investment. Yeah, so this looks like it's worked. So it's scraped every policy in the list. And again, we're back to, you know, saving the results. So the real work with web scraping is manually working through the source code of a web page. What's the idea of the section or the paragraph or the image that I'm interested in searching for those IDs, and then rationalising filtering that list and the actual scraping is reasonably easy to extract the information. It's kind of trying to navigate the structure and takes a bit of work. So we brought in a module called pandas earlier, I think it's named after the animal, I assume so. And basically, I'm just creating a data set in Python, which saves the list of results and that I showed you here, so it's just translating that into a data set. And and as you can see, now, it looks like A little bit better, we've put it into a tabular format. And you know, row one charity name, and then the policy in question. I mean, I could have had this, you know, structured differently, I could have had, you know, charity name policy one, policy two, I could have had different variables or columns across the top. As a social scientist, I just prefer working with it in this format here. So it's called long format. But that's, that's neither here nor there. You can structure it how you like, again, all I want to do is I want to save the file. And pandas makes that really easily. So there's a two underscore CSV method, convert this file here to CSV. And I just have an option specified, as well, but I could take that out. There's no trouble. So again, how do I know it's been created? Well, is there now a file? Yes, there is. Now as you can see, the eagle eyed among you will see that this doesn't it didn't exist previously. Again. Not forming a magic trick. This file has just been created. And but we want to look inside of it, does it actually contain what we want? Again, we can use pandas. This time we can read in the CSV, and some encoding just so Python can understand the contents. We take a look at it again. And you know, voila, the data does exist in bash and file.

Diarmuid McDonnel  
It so that's a bit of a whirlwind. I think that is exactly half an hour. That's the best I've done. This is fourth or fifth time doing this. So thank you very much. What have we learned? So, obviously, I've kind of whizzed through some of the code, but it's all there in that that link that I shared with you. So you can work through this in your own time as well. So we've learned how to import modules, you know, into Python, very simple task, but it's a necessary task. And we've learned how to request webpages hopefully you've you've realised that it's not actually that technical and require webpages are scraping web pages. The real work is understanding the structure. It's visually working through a going on. Okay, I need a paragraph here, but it's contained in a section, can I get a hold of that information? And we've read data in and out of different files, which is really good. And hopefully, but this is for you to say to me, that we've done all of this quite efficiently, very clearly and hopefully very effective. So hopefully, I've demonstrated that this works and, and hopefully in a very clear way, we've done a longer webinar about this, where, because we're targeting social scientists, we had to talk extensively about ethics. I'm sure probably some questions have come through in the chat or please submit them about the ethics of what we're doing. And the simple example is designed for web scraping, no trouble there. The charity regulator because it's a government website. The data is available under the open government licence. I think it's version two, instead of version three. And web scraping is not explicitly prohibited, and it's not allowed exploited. So it's probably a little bit of a grey area. But there's nothing that says you may not computationally scrape this web page. You're always allowed manually scrape a webpage, nobody can. You won't say nobody. I'm not a lawyer. But it's incredibly unlikely that anybody will be able to take any action against you for copying and pasting paragraphs or images, you know, onto your machine. Copyright law still applies. But the actual scraping can be done manually. It's when you start trying to do this automatically. So web scraping very powerful. It's a very simple technique. I hope you agree with that, you know, it can take hours to learn as opposed to days or weeks. It does take you into the realm of data protection. So if I wanted to scrape the trustees, which I do and have done, and that's personal data that brings me into The GDPR or whatever we're gonna have, you know, come January in the UK when you know, we leave, we properly leave the European Union. And websites have Terms of Service or Terms of Use. And, and they can explicitly say you cannot scrape this website. And, again, they would have to catch you doing it. But if if they do the terms of use, you know, constitutes a contract between you and the website. If it says you can't scrape, you're really, you know, potentially getting into into trouble. And there's other ethical issues as well. So as I said, we've done lots of other free kind of learning resources. There's a much more detailed web scraping webinar and notebook where we scrape COVID-19 data. Frankly, I'm a little sick of shoehorning COVID into everything I do at the moment. So I decided to do a bit of Moby Dick and charity policies but it is important But I think you can do too much of it. And and there's a really good book I quite like automate the boring stuff with Python. I mean, you can buy a hard copy, but it's, you know, it's it's open source and on the web. I think it's chapter 12. Yes, it's web scraping. So I would read that covers a lot of what I've done, but some extra things and gives you a bit more of the technical information underpinning web pages, as well. So last thing, yes. So everything I've showed you here is publicly available. So we've got a GitHub repository, and I'll post that in the chat as well. We've also done you know, some things on tomorrow about computational environments. You know, we've done an introduction to programming and we've done the web scraping today, and I've also done API's and recently as well, same deal, you know, it's an open source notebook you can work through yourself and there's Yeah, there's a YouTube webinar as well if you're interested. So I'll post that link but i think you know, that's enough for me. And so I will stop sharing the screen and I'm more than happy. I'm a little bit hungry but I'm, I'll stick around for quite as many questions as you want. So thank you very, very much.

Transcribed by https://otter.ai
